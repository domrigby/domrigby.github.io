<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script>                <div id="e3fa266d-eb83-4ffb-8038-96c74219d60f" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("e3fa266d-eb83-4ffb-8038-96c74219d60f")) {                    Plotly.newPlot(                        "e3fa266d-eb83-4ffb-8038-96c74219d60f",                        [{"customdata":[["Absolute Zero Reasoner","Reasoning via RL Fine-Tuning"],["Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment","Reasoning via RL Fine-Tuning"],["ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models","Reasoning via RL Fine-Tuning"],["Beyond the 80\u002f20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning","Reasoning via RL Fine-Tuning"],["Play to Generalize: Learning to Reason Through Game Play","Reasoning via RL Fine-Tuning"],["SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning","Reasoning via RL Fine-Tuning"],["FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning","Reasoning via RL Fine-Tuning"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Reasoning via RL Fine-Tuning","marker":{"color":"#636EFA","symbol":"circle"},"mode":"markers","name":"Reasoning via RL Fine-Tuning","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"5c\u002f9PmdL8DxUoO0+b41CP5LVvL5Vuhi\u002fsxEavw=="},"xaxis":"x","y":{"dtype":"f4","bdata":"WYQVQTuoF0Ew2BxBgZAZQawmEEH8fQ5BmcEXQQ=="},"yaxis":"y","type":"scatter"},{"customdata":[["\u03c0_{0.5}: a Vision-Language-Action Model with Open-World Generalization","Embodied Curriculum Learning"],["Synthetic Experience Replay","Embodied Curriculum Learning"],["Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (Demo Augmented RL)","Embodied Curriculum Learning"],["mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity","Embodied Curriculum Learning"],["KINETIX: INVESTIGATING THE TRAINING OF GENERAL AGENTS THROUGH OPEN-ENDED PHYSICS-BASED CONTROL TASKS","Embodied Curriculum Learning"],["Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning","Embodied Curriculum Learning"],["Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning","Embodied Curriculum Learning"],["Multi-Agent Diagnostics for Robustness via Illuminated  (MADRID)","Embodied Curriculum Learning"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Embodied Curriculum Learning","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"Embodied Curriculum Learning","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"D8aFPaWJbj5D7X++wHPovW3pSr9NSEi\u002flOSMvsqoZL8="},"xaxis":"x","y":{"dtype":"f4","bdata":"v3\u002fRQGZd50CbmNVAQNXbQM9J1EBSxuxALO3mQDJg4UA="},"yaxis":"y","type":"scatter"},{"customdata":[["TD-MPC2: Scalable, Robust World Models for Continuous Control","Reward and Generalisation"],["Ultimate Guide to Supervised Fine-Tuning","Reward and Generalisation"],["Spurious Rewards: Rethinking Training Signals in RLVR","Reward and Generalisation"],["CHIRPs: Change-Induced Regret Proxy Metrics for Lifelong Reinforcement Learning","Reward and Generalisation"],["Reinforcement Pre-Training","Reward and Generalisation"],["Reinforcement Learning Teachers of Test Time Scaling","Reward and Generalisation"],["Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards","Reward and Generalisation"],["Prioritised Level Replay","Reward and Generalisation"],["Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL","Reward and Generalisation"],["Model-Based Meta Automatic Curriculum Learning","Reward and Generalisation"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Reward and Generalisation","marker":{"color":"#00CC96","symbol":"circle"},"mode":"markers","name":"Reward and Generalisation","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"MMvBP+lN4T+\u002fbYc+YzK4Pqdlkj89xKA\u002fXbYSP1lmzT5ug\u002fw+4KJRPw=="},"xaxis":"x","y":{"dtype":"f4","bdata":"v2b8QIfOBkEe4wZBkq7\u002fQIJjBUEpC\u002fNAqHEEQa5s7kA5pwtB4xb7QA=="},"yaxis":"y","type":"scatter"},{"customdata":[["JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Age Reinforcement Learning","Scalable Multi-Agent RL"],["JaxMARL: Multi-Agent RL Environments and Algorithms in JAX","Scalable Multi-Agent RL"],["Training extremely large neural networks across thousands of GPUs by Jeremy Jordan","Scalable Multi-Agent RL"],["IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures","Scalable Multi-Agent RL"],["TorchBeast: A PyTorch Platform for Distributed RL","Scalable Multi-Agent RL"],["Distributed PPO Blog Post","Scalable Multi-Agent RL"],["Reinforcement Learning with Docker","Scalable Multi-Agent RL"],["Illuminating search spaces by mapping elites","Scalable Multi-Agent RL"],["How to scale RL to 10^26 FLOPs blog by Jack Morris","Scalable Multi-Agent RL"],["INTELLECT-1 Technical Report","Scalable Multi-Agent RL"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Scalable Multi-Agent RL","marker":{"color":"#AB63FA","symbol":"circle"},"mode":"markers","name":"Scalable Multi-Agent RL","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"vMeZP+zf8T+dyyJAxHsGQE+5F0BEXxNA0VIMQB5m6z8YnidAUD0hQA=="},"xaxis":"x","y":{"dtype":"f4","bdata":"94\u002fSQKslzkBcb\u002fdA7MDoQMJK0kCSaeNABtjVQKCS4kCFa+lAnAUBQQ=="},"yaxis":"y","type":"scatter"},{"customdata":[["Illusion of Thinking: Understanding Strengths and Limitations of Large Reasoning Models (LRMs)","Foundation Model Introspection"],["Self-Adapting Language Models","Foundation Model Introspection"],["How Visual Representations Map to Language Feature Space in Multimodal LLMs","Foundation Model Introspection"],["Self-Supervised Video Models Enable Understanding, Prediction and Planning (V-JEPA)","Foundation Model Introspection"],["Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time","Foundation Model Introspection"],["What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models","Foundation Model Introspection"],["Deep Dive into Yann LeCun\u2019s JEPA by Rohit Bandaru","Foundation Model Introspection"],["All AI Models Might Be The Same by Jack Morris","Foundation Model Introspection"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Foundation Model Introspection","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"Foundation Model Introspection","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"l718Py0C8z9jg9w\u002fW+kAQCoQE0CJaaw\u002fZiqvP0af8z8="},"xaxis":"x","y":{"dtype":"f4","bdata":"+DAeQYCyDkFvgRxBhO8XQe47CUHMHRNB\u002f+McQUYuIEE="},"yaxis":"y","type":"scatter"},{"customdata":[["Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions","Open-Endedness and Self-Play"],["Agents of Change: Self-Evolving LLM Agents for Strategic Planning","Open-Endedness and Self-Play"],["Superintelligence From First Principles (blog post)","Open-Endedness and Self-Play"],["Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach","Open-Endedness and Self-Play"],["Automatic Curriculum Design for Zero-Shot Human AI Coordination","Open-Endedness and Self-Play"],["OMNI-EPIC: Open-Endedness Via Models of Human Notions of Interestingness With Environments Programmed In Code","Open-Endedness and Self-Play"],["Open-Endedness is Essential for Artificial Superhuman Intelligence","Open-Endedness and Self-Play"],["Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models","Open-Endedness and Self-Play"],["TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play","Open-Endedness and Self-Play"],["Eurekaverse: Environment Curriculum Generation via Large Language Models","Open-Endedness and Self-Play"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Open-Endedness and Self-Play","marker":{"color":"#19D3F3","symbol":"circle"},"mode":"markers","name":"Open-Endedness and Self-Play","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"ZPOkv5lx1r9WS9a\u002fK420v1n\u002fz7\u002fvgRq\u002fo2C2vzCq6L85nay\u002fTtx3vw=="},"xaxis":"x","y":{"dtype":"f4","bdata":"rswDQd5CAEGvXglB85X4QNNh7UAdjQRBOYoLQbBO\u002fUAgyulApV\u002f8QA=="},"yaxis":"y","type":"scatter"},{"fill":"toself","fillcolor":"rgba(99,110,250,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(99,110,250,1)","width":1},"name":"Convex Hulls","showlegend":true,"visible":true,"x":{"dtype":"f4","bdata":"sxEav1W6GL\u002flz\u002f0+b41CP1Sg7T6zERq\u002f"},"y":{"dtype":"f4","bdata":"mcEXQfx9DkFZhBVBgZAZQTDYHEGZwRdB"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(239,85,59,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(239,85,59,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"D8aFPaWJbj5NSEi\u002fyqhkv23pSr8PxoU9"},"y":{"dtype":"f4","bdata":"v3\u002fRQGZd50BSxuxAMmDhQM9J1EC\u002ff9FA"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(0,204,150,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(0,204,150,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"v22HPllmzT49xKA\u002fMMvBP+lN4T9ug\u002fw+v22HPg=="},"y":{"dtype":"f4","bdata":"HuMGQa5s7kApC\u002fNAv2b8QIfOBkE5pwtBHuMGQQ=="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(171,99,250,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(171,99,250,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"UD0hQLzHmT\u002fs3\u002fE\u002fT7kXQBieJ0BQPSFA"},"y":{"dtype":"f4","bdata":"nAUBQfeP0kCrJc5AwkrSQIVr6UCcBQFB"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(255,161,90,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(255,161,90,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"Rp\u002fzP5e9fD+Jaaw\u002fKhATQEaf8z8="},"y":{"dtype":"f4","bdata":"Ri4gQfgwHkHMHRNB7jsJQUYuIEE="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(25,211,243,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(25,211,243,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"OZ2sv++BGr+jYLa\u002fVkvWvzCq6L9Z\u002f8+\u002fOZ2svw=="},"y":{"dtype":"f4","bdata":"IMrpQB2NBEE5igtBr14JQbBO\u002fUDTYe1AIMrpQA=="},"type":"scatter"},{"hoverinfo":"skip","line":{"dash":"dash","width":1},"mode":"lines","name":"Reading Order","visible":"legendonly","x":{"dtype":"f4","bdata":"5c\u002f9Pg\u002fGhT0wy8E\u002fvMeZP6WJbj5nS\u002fA86U3hP0Ptf76\u002fbYc+VKDtPuzf8T+XvXw\u002fZPOkv2MyuD6nZZI\u002fPcSgP2+NQj9dthI\u002fktW8vsBz6L1ZZs0+LQLzP5lx1r9t6Uq\u002fVkvWv01ISL9jg9w\u002fK420v1n\u002fz7\u002fvgRq\u002fW+kAQKNgtr9Vuhi\u002fncsiQMR7BkBPuRdARF8TQNFSDECzERq\u002fHmbrPzCq6L8YnidAUD0hQCoQE0CU5Iy+iWmsP2Yqrz9Gn\u002fM\u002fyqhkv26D\u002fD45nay\u002f4KJRP07cd78="},"y":{"dtype":"f4","bdata":"WYQVQb9\u002f0UC\u002fZvxA94\u002fSQGZd50A7qBdBh84GQZuY1UAe4wZBMNgcQaslzkD4MB5BrswDQZKu\u002f0CCYwVBKQvzQIGQGUGocQRBrCYQQUDV20CubO5AgLIOQd5CAEHPSdRAr14JQVLG7EBvgRxB85X4QNNh7UAdjQRBhO8XQTmKC0H8fQ5BXG\u002f3QOzA6EDCStJAkmnjQAbY1UCZwRdBoJLiQLBO\u002fUCFa+lAnAUBQe47CUEs7eZAzB0TQf\u002fjHEFGLiBBMmDhQDmnC0EgyulA4xb7QKVf\u002fEA="},"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"\u003cb\u003eDim 1\u003c\u002fb\u003e"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"\u003cb\u003eDim 2\u003c\u002fb\u003e"}},"legend":{"title":{"text":"theme"},"tracegroupgap":0,"orientation":"h","yanchor":"bottom","y":-0.2,"xanchor":"center","x":0.5},"title":{"text":"\u003cb\u003eUMAP Dimension Reduction of Description Embeddings\u003c\u002fb\u003e","x":0.5},"margin":{"b":150},"annotations":[{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Embodied Curriculum Learning","x":-0.3510654866695404,"xanchor":"center","y":6.951789855957031,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Foundation Model Introspection","x":1.6924232244491577,"xanchor":"center","y":9.458996772766113,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Open-Endedness and Self-Play","x":-1.3836548328399658,"xanchor":"center","y":8.014005661010742,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Reasoning via RL Fine-Tuning","x":0.025985896587371826,"xanchor":"center","y":9.374844551086426,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Reward and Generalisation","x":0.858453631401062,"xanchor":"center","y":8.09687614440918,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Scalable Multi-Agent RL","x":2.1579411029815674,"xanchor":"center","y":7.082758903503418,"yanchor":"middle"}]},                        {"responsive": true}                    )                };            </script>        </div>
</body>
</html>