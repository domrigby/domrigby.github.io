<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script>                <div id="1b773cdf-d3c2-43a6-af95-293cab121328" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("1b773cdf-d3c2-43a6-af95-293cab121328")) {                    Plotly.newPlot(                        "1b773cdf-d3c2-43a6-af95-293cab121328",                        [{"customdata":[["Absolute Zero Reasoner","LLM Reasoning and Meta-RL"],["Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment","LLM Reasoning and Meta-RL"],["ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models","LLM Reasoning and Meta-RL"],["Illusion of Thinking: Understanding Strengths and Limitations of Large Reasoning Models (LRMs)","LLM Reasoning and Meta-RL"],["Beyond the 80\u002f20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning","LLM Reasoning and Meta-RL"],["Play to Generalize: Learning to Reason Through Game Play","LLM Reasoning and Meta-RL"],["SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning","LLM Reasoning and Meta-RL"],["Deep Dive into Yann LeCun\u2019s JEPA by Rohit Bandaru","LLM Reasoning and Meta-RL"],["GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning","LLM Reasoning and Meta-RL"],["Hierarchical Reasoning Model","LLM Reasoning and Meta-RL"],["Sable: a Performant, Efficient and Scalable Sequence Model for MARL","LLM Reasoning and Meta-RL"],["ProRL V2 - Prolonged Training Validates RL Scaling Laws","LLM Reasoning and Meta-RL"],["On the Design of KL-Regularised Policy Gradient Algorithms for LLM Reasoning","LLM Reasoning and Meta-RL"],["RL Grokking Recipe- How Can We Enable LLMs to Solve Previously Unsolvable Tasks","LLM Reasoning and Meta-RL"],["Long-Horizon Perception Requires Re-Thinking Recurrence","LLM Reasoning and Meta-RL"],["How Far Can Off-Policy RL Reach with Stale Data on LLMs?","LLM Reasoning and Meta-RL"],["Evaluating Long Context (Reasoning) Ability","LLM Reasoning and Meta-RL"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"LLM Reasoning and Meta-RL","marker":{"color":"#636EFA","symbol":"circle"},"mode":"markers","name":"LLM Reasoning and Meta-RL","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"m1UgQEZsB0ADIhlAIbAdQJNHJUDUUgFAmDULQFcCI0BioRBAU\u002fgPQEAHY0ChN0dAeLI4QKlqG0Bo+EVAB6hcQIWPIkA="},"xaxis":"x","y":{"dtype":"f4","bdata":"yyf9PgZDEj0aga89Slkav4sw970p7gc\u002fIUswP9X\u002fI7+JMvG+tDOgPp8NgT3BfP0+z0tTPyzg6j5ZIPy+FrAWP9Cinr4="},"yaxis":"y","type":"scatter"},{"customdata":[["\u03c0_{0.5}: a Vision-Language-Action Model with Open-World Generalization","RL Benchmarks and World Models"],["TD-MPC2: Scalable, Robust World Models for Continuous Control","RL Benchmarks and World Models"],["Synthetic Experience Replay","RL Benchmarks and World Models"],["Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (Demo Augmented RL)","RL Benchmarks and World Models"],["Spurious Rewards: Rethinking Training Signals in RLVR","RL Benchmarks and World Models"],["CHIRPs: Change-Induced Regret Proxy Metrics for Lifelong Reinforcement Learning","RL Benchmarks and World Models"],["Reinforcement Pre-Training","RL Benchmarks and World Models"],["Reinforcement Learning Teachers of Test Time Scaling","RL Benchmarks and World Models"],["Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards","RL Benchmarks and World Models"],["mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity","RL Benchmarks and World Models"],["Prioritised Level Replay","RL Benchmarks and World Models"],["Multi-Agent Diagnostics for Robustness via Illuminated  (MADRID)","RL Benchmarks and World Models"],["Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL","RL Benchmarks and World Models"],["Model-Based Meta Automatic Curriculum Learning","RL Benchmarks and World Models"],["DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning","RL Benchmarks and World Models"],["Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation","RL Benchmarks and World Models"],["Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning","RL Benchmarks and World Models"],["Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels","RL Benchmarks and World Models"],["Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning","RL Benchmarks and World Models"],["SMX: Sequential Monte Carlo Planning for Expert Iteration","RL Benchmarks and World Models"],["Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space","RL Benchmarks and World Models"],["RL\u2019S RAZOR: WHY ONLINE REINFORCEMENT LEARNING FORGETS LESS","RL Benchmarks and World Models"],["Debugging RL, Without the Agonizing Pain","RL Benchmarks and World Models"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"RL Benchmarks and World Models","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"RL Benchmarks and World Models","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"VH6hQGKHiEAje4ZAvfWcQFh4aEB6MW1AA3N7QDwMj0AUxWZACJScQHOtlED115JAx9lUQI1MhkCy7ZpACsyPQIdSikDMbWdAdEOGQHy6kkCY6oBADhlsQP18fkA="},"xaxis":"x","y":{"dtype":"f4","bdata":"Md9ePxwjuD98R0M\u002f6OQzP1GUiD\u002f60IE\u002fbR7WPz5\u002fA0Bhnq4\u002flmh3Pyj3DEDuKjo\u002fsHabP4Hb4z\u002flGw1AXPauPg2JpD5jINI\u002f5OboPk69tD9ij6E\u002f7IIiP7kuNT8="},"yaxis":"y","type":"scatter"},{"customdata":[["JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Age Reinforcement Learning","RL Algorithms and Applications"],["IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures","RL Algorithms and Applications"],["TorchBeast: A PyTorch Platform for Distributed RL","RL Algorithms and Applications"],["Distributed PPO Blog Post","RL Algorithms and Applications"],["Illuminating search spaces by mapping elites","RL Algorithms and Applications"],["Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning","RL Algorithms and Applications"],["Benchmarking Population-Based Reinforcement Learning across Robotic Tasks with GPU-Accelerated Simulation","RL Algorithms and Applications"],["Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization","RL Algorithms and Applications"],["Communicative Reinforcement Learning Agents for Landmark Detection in Brain Images","RL Algorithms and Applications"],["Intelligent Railway Capacity and Traffic Management Using Multi-Agent Deep Reinforcement Learning","RL Algorithms and Applications"],["GOAL: A Generalist Combinatorial Optimization Agent Learner","RL Algorithms and Applications"],["Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning","RL Algorithms and Applications"],["The 37 Implementation Details of Proximal Policy Optimization","RL Algorithms and Applications"],["Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning","RL Algorithms and Applications"],["DD-PPO: LEARNING NEAR-PERFECT POINTGOAL NAVIGATORS FROM 2.5 BILLION FRAMES","RL Algorithms and Applications"],["In-Context Reinforcement Learning for Variable Action Spaces","RL Algorithms and Applications"],["Polychromic Objectives for Reinforcement Learning","RL Algorithms and Applications"],["Attention, Learn to Solve Routing Problems","RL Algorithms and Applications"],["Deep Reinforcement Learning Guided Improvement Heuristic For Job Shop Scheduling","RL Algorithms and Applications"],["POMO: Policy Optimization with Multiple Optima for Reinforcement Learning","RL Algorithms and Applications"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"RL Algorithms and Applications","marker":{"color":"#00CC96","symbol":"circle"},"mode":"markers","name":"RL Algorithms and Applications","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"F2zPQDeonUA9M5xA7WG7QNFfz0Bsnr1A7FiTQIisx0BRooZASTKfQKwbvkBDTZJAKKfFQAupk0DIYYhA3SGsQND4t0AlrrJAGQSpQIw\u002fzkA="},"xaxis":"x","y":{"dtype":"f4","bdata":"FL+sPoJ0J7\u002f3wme\u002fvyeBv49onL7pNAo+qRAAvjrRxzzOQg+\u002fwncsPQ\u002fInb1qPJm+xCjzvmNF7b6Ox6K+tqYOP9rJCj65J7i+pef9veHfOL4="},"yaxis":"y","type":"scatter"},{"customdata":[["Ultimate Guide to Supervised Fine-Tuning","Model Training and Adaptation"],["Self-Adapting Language Models","Model Training and Adaptation"],["How Visual Representations Map to Language Feature Space in Multimodal LLMs","Model Training and Adaptation"],["Self-Supervised Video Models Enable Understanding, Prediction and Planning (V-JEPA)","Model Training and Adaptation"],["INTELLECT-1 Technical Report","Model Training and Adaptation"],["Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time","Model Training and Adaptation"],["All AI Models Might Be The Same by Jack Morris","Model Training and Adaptation"],["A Gentle Introduction to Graph Neural Networks","Model Training and Adaptation"],["Nvidia Docs: Optimising Memory Limited Layers","Model Training and Adaptation"],["Current Best Practices for Training LLMs from Scratch","Model Training and Adaptation"],["Flash Attention Blog Post","Model Training and Adaptation"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Model Training and Adaptation","marker":{"color":"#AB63FA","symbol":"circle"},"mode":"markers","name":"Model Training and Adaptation","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"stJcQMOkQUAzFElAftl0QI2dSkCSqF1ARB+CQCY\u002fiUBnrGxA9z4+QBnNc0A="},"xaxis":"x","y":{"dtype":"f4","bdata":"bk\u002fLv1Jswr\u002fhcY6\u002fIzC7v3DM6L+KygXATN0FwDKp\u002fb8Rwh7AURXUv+6lKMA="},"yaxis":"y","type":"scatter"},{"customdata":[["JaxMARL: Multi-Agent RL Environments and Algorithms in JAX","Systems and GPU Optimisation"],["Training extremely large neural networks across thousands of GPUs by Jeremy Jordan","Systems and GPU Optimisation"],["Reinforcement Learning with Docker","Systems and GPU Optimisation"],["How to scale RL to 10^26 FLOPs blog by Jack Morris","Systems and GPU Optimisation"],["A Survey of Graph Transformers: Architectures, Theories and Applications","Systems and GPU Optimisation"],["Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX","Systems and GPU Optimisation"],["Maarten Grootendorst's blog post on A Visual Guide to Quantization","Systems and GPU Optimisation"],["Following Jax MNIST tutorials","Systems and GPU Optimisation"],["Compiling machine learning programs via high-level tracing","Systems and GPU Optimisation"],["How to think about GPUs by Google DeepMind","Systems and GPU Optimisation"],["PyTorch Internals by Edward Wang","Systems and GPU Optimisation"],["What is Torch Compile?","Systems and GPU Optimisation"],["JIT Compilation in JAX","Systems and GPU Optimisation"],["CUDA Study Log 4: Optimizing Constrained Decoding with Triton Kernel","Systems and GPU Optimisation"],["Accelerating PyTorch with CUDA Graphs","Systems and GPU Optimisation"],["PyTorch Performance Tuning Guide","Systems and GPU Optimisation"],["Nvidia Docs: GPU Performance Fundamentals","Systems and GPU Optimisation"],["Nvidia Docs: Matrix Multiplication and Quantisation Background","Systems and GPU Optimisation"],["Speeding Up Graph Learning Models with PyG and torch.compile","Systems and GPU Optimisation"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Systems and GPU Optimisation","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"Systems and GPU Optimisation","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"HQewQKG8j0DLVqVAr\u002fuGQAHAlUCoK7JAD5eGQOAuqkCwDYxAdS6SQFZSmEBEnI5ATdyrQFf9lEB4HJtAlW2CQHEInUC8gZRAUSaRQA=="},"xaxis":"x","y":{"dtype":"f4","bdata":"vwLuv7wPZMCDZzLALTBZwF\u002ffFsDKAwPAPQo\u002fwPRxAMDt6VvANoVswKScQMC5Kj7AphQSwAFta8DFbVjAG1I7wEjTasDRXm\u002fAhC80wA=="},"yaxis":"y","type":"scatter"},{"customdata":[["Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions","Curriculum and Open-Ended Learning"],["Agents of Change: Self-Evolving LLM Agents for Strategic Planning","Curriculum and Open-Ended Learning"],["KINETIX: INVESTIGATING THE TRAINING OF GENERAL AGENTS THROUGH OPEN-ENDED PHYSICS-BASED CONTROL TASKS","Curriculum and Open-Ended Learning"],["Superintelligence From First Principles (blog post)","Curriculum and Open-Ended Learning"],["Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning","Curriculum and Open-Ended Learning"],["Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach","Curriculum and Open-Ended Learning"],["Automatic Curriculum Design for Zero-Shot Human AI Coordination","Curriculum and Open-Ended Learning"],["OMNI-EPIC: Open-Endedness Via Models of Human Notions of Interestingness With Environments Programmed In Code","Curriculum and Open-Ended Learning"],["Open-Endedness is Essential for Artificial Superhuman Intelligence","Curriculum and Open-Ended Learning"],["FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning","Curriculum and Open-Ended Learning"],["Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models","Curriculum and Open-Ended Learning"],["What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models","Curriculum and Open-Ended Learning"],["TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play","Curriculum and Open-Ended Learning"],["Eurekaverse: Environment Curriculum Generation via Large Language Models","Curriculum and Open-Ended Learning"],["Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation","Curriculum and Open-Ended Learning"],["Open-Ended Learning Leads to Generally Capable Agents","Curriculum and Open-Ended Learning"],["The Bitter Lesson","Curriculum and Open-Ended Learning"],["Efficiently Quantifying Individual Agent Importance in Cooperative MARL","Curriculum and Open-Ended Learning"],["Training Agents Inside of Scalable World Models","Curriculum and Open-Ended Learning"],["The Bitter Lesson's Bitter Lesson","Curriculum and Open-Ended Learning"],["9 Tips for Managing & Leading an Engineering Team","Curriculum and Open-Ended Learning"],["22 Project Management Tools & Techniques for Project Managers","Curriculum and Open-Ended Learning"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Curriculum and Open-Ended Learning","marker":{"color":"#19D3F3","symbol":"circle"},"mode":"markers","name":"Curriculum and Open-Ended Learning","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"Hd\u002fDQHtPyEBg2qhAF93QQL4qpUCPCbhA+g27QPB92EA6OtFACyzQQNSvyUCjXK9A0+6vQIKFtUDogchAHM27QFnv3kAQEb5AhSOqQGfG30C4sspApe\u002fTQA=="},"xaxis":"x","y":{"dtype":"f4","bdata":"8E8NQAhZEEBFN7c\u002f0qHsP6AGE0CZpQ5A4XYJQC29qT+gdvs\u002fKQm1P8JMAkD+rIQ\u002fFNcRQLOx6j\u002fDLC8\u002ftZXaP2wsyj+0tJE\u002fE5KcPyFtjj+CFKM\u002fRIFcPw=="},"yaxis":"y","type":"scatter"},{"fill":"toself","fillcolor":"rgba(99,110,250,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(99,110,250,1)","width":1},"name":"Convex Hulls","showlegend":true,"visible":true,"x":{"dtype":"f4","bdata":"aPhFQEAHY0AHqFxAeLI4QJg1C0DUUgFARmwHQGKhEEAhsB1AVwIjQGj4RUA="},"y":{"dtype":"f4","bdata":"WSD8vp8NgT0WsBY\u002fz0tTPyFLMD8p7gc\u002fBkMSPYky8b5KWRq\u002f1f8jv1kg\u002fL4="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(239,85,59,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(239,85,59,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"VH6hQLLtmkBzrZRAzG1nQMfZVEAOGWxAh1KKQArMj0C99ZxAVH6hQA=="},"y":{"dtype":"f4","bdata":"Md9eP+UbDUAo9wxAYyDSP7B2mz\u002fsgiI\u002fDYmkPlz2rj7o5DM\u002fMd9ePw=="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(0,204,150,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(0,204,150,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"F2zPQN0hrEDIYYhAUaKGQD0znEDtYbtA0V\u002fPQBdsz0A="},"y":{"dtype":"f4","bdata":"FL+sPramDj+Ox6K+zkIPv\u002ffCZ7+\u002fJ4G\u002fj2icvhS\u002frD4="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(171,99,250,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(171,99,250,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"9z4+QBnNc0AmP4lAftl0QDMUSUD3Pj5A"},"y":{"dtype":"f4","bdata":"URXUv+6lKMAyqf2\u002fIzC7v+Fxjr9RFdS\u002f"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(255,161,90,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(255,161,90,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"cQidQKgrskAdB7BAAcCVQJVtgkCv+4ZAdS6SQLyBlEBxCJ1A"},"y":{"dtype":"f4","bdata":"SNNqwMoDA8C\u002fAu6\u002fX98WwBtSO8AtMFnANoVswNFeb8BI02rA"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(25,211,243,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(25,211,243,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"e0\u002fIQL4qpUBg2qhAhSOqQKNcr0DogchApe\u002fTQGfG30BZ795Ae0\u002fIQA=="},"y":{"dtype":"f4","bdata":"CFkQQKAGE0BFN7c\u002fE5KcP\u002f6shD\u002fDLC8\u002fRIFcPyFtjj9sLMo\u002fCFkQQA=="},"type":"scatter"},{"hoverinfo":"skip","line":{"dash":"dash","width":1},"mode":"lines","name":"Reading Order","visible":"legendonly","x":{"dtype":"f4","bdata":"m1UgQFR+oUBih4hAF2zPQCN7hkBGbAdAstJcQL31nEBYeGhAAyIZQB0HsEAhsB1AHd\u002fDQHoxbUADc3tAPAyPQJNHJUAUxWZA1FIBQAiUnEBzrZRAw6RBQHtPyEBg2qhAF93QQL4qpUAzFElAjwm4QPoNu0DwfdhAftl0QDo60UCYNQtAobyPQDeonUA9M5xA7WG7QMtWpUALLNBA0V\u002fPQNSvyUCv+4ZAjZ1KQJKoXUBsnr1Ao1yvQFcCI0BEH4JA9deSQMfZVEDT7q9AjUyGQIKFtUDsWJNAsu2aQIisx0AKzI9AUaKGQEkyn0CHUopAdEOGQGKhEEDMbWdAU\u002fgPQKwbvkABwJVAJj+JQOiByEAczbtAQAdjQHy6kkChN0dAQ02SQCinxUALqZNAmOqAQMhhiEBZ795A3SGsQKgrskAQEb5AD5eGQOAuqkCwDYxAdS6SQA4ZbEBWUphARJyOQE3cq0BX\u002fZRAeBybQJVtgkBxCJ1AZ6xsQLyBlEB4sjhA9z4+QND4t0CpahtA\u002fXx+QBnNc0CFI6pAaPhFQCWuskAZBKlAUSaRQAeoXECMP85AhY8iQGfG30C4sspApe\u002fTQA=="},"y":{"dtype":"f4","bdata":"yyf9PjHfXj8cI7g\u002fFL+sPnxHQz8GQxI9bk\u002fLv+jkMz9RlIg\u002fGoGvPb8C7r9KWRq\u002f8E8NQPrQgT9tHtY\u002fPn8DQIsw971hnq4\u002fKe4HP5Zodz8o9wxAUmzCvwhZEEBFN7c\u002f0qHsP6AGE0DhcY6\u002fmaUOQOF2CUAtvak\u002fIzC7v6B2+z8hSzA\u002fvA9kwIJ0J7\u002f3wme\u002fvyeBv4NnMsApCbU\u002fj2icvsJMAkAtMFnAcMzov4rKBcDpNAo+\u002fqyEP9X\u002fI79M3QXA7io6P7B2mz8U1xFAgdvjP7Ox6j+pEAC+5RsNQDrRxzxc9q4+zkIPv8J3LD0NiaQ+5OboPoky8b5jINI\u002ftDOgPg\u002fInb1f3xbAMqn9v8MsLz+1ldo\u002fnw2BPU69tD\u002fBfP0+ajyZvsQo875jRe2+Yo+hP47Hor5sLMo\u002ftqYOP8oDA8C0tJE\u002fPQo\u002fwPRxAMDt6VvANoVswOyCIj+knEDAuSo+wKYUEsABbWvAxW1YwBtSO8BI02rAEcIewNFeb8DPS1M\u002fURXUv9rJCj4s4Oo+uS41P+6lKMATkpw\u002fWSD8vrknuL6l5\u002f29hC80wBawFj\u002fh3zi+0KKeviFtjj+CFKM\u002fRIFcPw=="},"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"\u003cb\u003eDim 1\u003c\u002fb\u003e"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"\u003cb\u003eDim 2\u003c\u002fb\u003e"}},"legend":{"title":{"text":"theme"},"tracegroupgap":0,"orientation":"h","yanchor":"bottom","y":-0.2,"xanchor":"center","x":0.5},"title":{"text":"\u003cb\u003eUMAP Dimension Reduction of Description Embeddings\u003c\u002fb\u003e","x":0.5},"margin":{"b":150},"annotations":[{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Curriculum and Open-Ended Learning","x":6.09149169921875,"xanchor":"center","y":1.6312896013259888,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"LLM Reasoning and Meta-RL","x":2.609877586364746,"xanchor":"center","y":0.11421237885951996,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Model Training and Adaptation","x":3.5371124744415283,"xanchor":"center","y":-1.8579777479171753,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"RL Algorithms and Applications","x":5.429157257080078,"xanchor":"center","y":-0.2311898022890091,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"RL Benchmarks and World Models","x":4.224510669708252,"xanchor":"center","y":1.1660727262496948,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Systems and GPU Optimisation","x":4.754977703094482,"xanchor":"center","y":-2.979343891143799,"yanchor":"middle"}]},                        {"responsive": true}                    )                };            </script>        </div>
</body>
</html>