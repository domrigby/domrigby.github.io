[
  {
    "title": "Absolute Zero Reasoner",
    "date_read": "2025-05-23",
    "link": "LLM_reinforcement_learning/AbsoluteZeroReasoner.md",
    "summary": "Addresses enabling language models to learn reasoning from scratch without demonstrations. Introduces a reinforcement learning framework starting from zero examples and iteratively refining decision policies. Demonstrates improved logical coherence and problem-solving accuracy in diverse reasoning tasks."
  },
  {
    "title": "π_{0.5}: a Vision-Language-Action Model with Open-World Generalization",
    "date_read": "2025-05-24",
    "link": "robotics/Pi0.5VLA.md",
    "summary": "Targets robust perception and action in unstructured environments. Proposes π_{0.5}, a model integrating visual, linguistic, and motor modules with adaptive planning. Demonstrates generalization to novel real-world tasks, achieving stable performance across varied scenarios."
  },
  {
    "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
    "date_read": "2025-05-25",
    "link": "non_LLM_reinforcement_learning/TDMPC2.md",
    "summary": "Addresses sample inefficiency in model-based control. Introduces TD-MPC2, which fuses predictive world models with temporal-difference updates to guide action selection. Demonstrates faster learning and greater robustness on continuous control benchmarks compared to prior methods."
  },
  {
    "title": "JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Age Reinforcement Learning",
    "date_read": "2025-05-26",
    "link": "marl/JointPPO.md",
    "summary": "Investigates coordination in multi-agent settings using proximal policy optimization. Presents JointPPO, extending PPO to jointly optimize across agents with shared objectives. Demonstrates faster convergence and improved collaborative behaviors in cooperative control environments."
  },
  {
    "title": "Synthetic Experience Replay",
    "date_read": "2025-05-29",
    "link": "non_LLM_reinforcement_learning/SyntheticExperienceReplay.md",
    "summary": "Addresses limited real-data interactions in reinforcement learning. Proposes synthetic experience replay, generating artificial transitions to augment training. Demonstrates accelerated policy improvement and enhanced stability on standard control and navigation tasks."
  },
  {
    "title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment",
    "date_read": "2025-05-30",
    "link": "LLM_reinforcement_learning/MultiTurnCreditAssignmentLLMRL.md",
    "summary": "Targets inconsistent multi-turn reasoning in conversational agents. Introduces a turn-level credit assignment mechanism that allocates feedback to individual inference steps. Demonstrates more coherent dialogue flows and higher success rates on complex conversational benchmarks."
  },
  {
    "title": "Ultimate Guide to Supervised Fine-Tuning",
    "date_read": "2025-06-01",
    "link": "general_training/UltimateGuideToSFT.md",
    "summary": "Addresses challenges in adapting pre-trained models to specific tasks. Outlines a structured supervised fine-tuning pipeline involving curated data selection, learning-rate schedules, and evaluation loops. Demonstrates consistent performance gains and improved generalization across varied downstream tasks."
  },
  {
    "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (Demo Augmented RL)",
    "date_read": "2025-06-02",
    "link": "non_LLM_reinforcement_learning/DemoAugmentedRL.md",
    "summary": "Targets fine-grained robotic manipulation. Combines deep reinforcement learning with demonstration augmentation to guide exploration. Demonstrates accelerated skill acquisition and higher manipulation precision in complex dexterous tasks compared to pure RL."
  },
  {
    "title": "Spurious Rewards: Rethinking Training Signals in RLVR",
    "date_read": "2025-06-03",
    "link": "LLM_reinforcement_learning/SpuriousRewardsRL.md",
    "summary": "Examines misleading feedback in reinforcement learning for vision-language tasks. Analyzes how spurious reward signals impair policy learning. Proposes refined reward shaping and gating mechanisms to filter noise. Demonstrates more stable training and improved task reliability."
  },
  {
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "date_read": "2025-06-04",
    "link": "LLM_reinforcement_learning/ProlongedRL.md",
    "summary": "Addresses limited multi-step inference in language models. Introduces ProRL, leveraging extended reinforcement learning cycles to deepen reasoning capacity. Demonstrates that prolonged training without architectural changes enhances complex problem-solving and inference depth."
  },
  {
    "title": "JaxMARL: Multi-Agent RL Environments and Algorithms in JAX",
    "date_read": "2025-06-05",
    "link": "marl/JaxMARL.md",
    "summary": "Addresses efficient multi-agent experimentation. Presents JaxMARL, a collection of cooperative and competitive environments and algorithms implemented in JAX. Demonstrates accelerated experimentation and scalable training, enabling rapid development of multi-agent strategies on modern hardware."
  },
  {
    "title": "Illusion of Thinking: Understanding Strengths and Limitations of Large Reasoning Models (LRMs)",
    "date_read": "2025-06-08",
    "link": "LLM_reinforcement_learning/IllusionOfThinking.md",
    "summary": "Investigates actual reasoning capabilities of advanced language models. Analyzes performance on benchmark tasks to differentiate true inference from pattern matching. Proposes a diagnostic framework revealing key strengths and failure modes alongside effective use cases."
  },
  {
    "title": "CHIRPs: Change-Induced Regret Proxy Metrics for Lifelong Reinforcement Learning",
    "date_read": "2025-06-09",
    "link": "non_LLM_reinforcement_learning/CHIRPLifeLongRL.md",
    "summary": "Addresses performance evaluation in continual learning. Introduces CHIRPs, proxy metrics quantifying regret from environmental shifts. Demonstrates that CHIRPs reliably predict long-term performance drops and guide adaptive policy updates for stable lifelong learning."
  },
  {
    "title": "Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions",
    "date_read": "2025-06-09",
    "link": "open_endedness_and_auto_curriculums/EnhancedPOETOpenEndedLearning.md",
    "summary": "Targets stagnation in open-ended learning frameworks. Proposes Enhanced POET, which continuously generates novel environments and corresponding solutions without predefinition. Demonstrates emergent curriculum complexity and improved agent versatility across diverse tasks."
  },
  {
    "title": "Reinforcement Pre-Training",
    "date_read": "2025-06-10",
    "link": "LLM_reinforcement_learning/RLPretraining.md",
    "summary": "Addresses inefficiencies in model initialization for sequential decision-making. Introduces reinforcement pre-training, using simulated interactions to prime model weights before task-specific learning. Demonstrates faster convergence and higher initial performance across varied control and reasoning challenges."
  },
  {
    "title": "Reinforcement Learning Teachers of Test Time Scaling",
    "date_read": "2025-06-11",
    "link": "LLM_reinforcement_learning/CreatingRLTeachers.md",
    "summary": "Targets dynamic computational constraints at inference. Proposes a teacher-student reinforcement learning scheme that trains guidance policies for test-time model scaling. Demonstrates improved trade-offs between efficiency and accuracy under varying resource budgets."
  },
  {
    "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
    "date_read": "2025-06-11",
    "link": "LLM_reinforcement_learning/TokenEntropyRLVR.md",
    "summary": "Investigates the influence of rare inputs in language model training. Shows that emphasizing high-entropy minority tokens during reinforcement learning boosts reasoning robustness. Demonstrates enhanced inference accuracy and generalization on complex reasoning benchmarks."
  },
  {
    "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards",
    "date_read": "2025-06-12",
    "link": "LLM_reinforcement_learning/WritingZeroNonVerifiableRewards.md",
    "summary": "Addresses learning on tasks lacking explicit feedback. Introduces Writing-Zero, which decomposes tasks into subtasks with proxy reward functions. Demonstrates reliable policy learning in scenarios where direct performance metrics are unavailable."
  },
  {
    "title": "Play to Generalize: Learning to Reason Through Game Play",
    "date_read": "2025-06-16",
    "link": "LLM_reinforcement_learning/ReasoningThroughGames.md",
    "summary": "Explores interactive gameplay to foster reasoning adaptability. Trains models on diverse game environments, encouraging transfer of inference strategies. Demonstrates improved performance on novel reasoning tasks beyond the training games."
  },
  {
    "title": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity",
    "date_read": "2025-06-17",
    "link": "non_LLM_reinforcement_learning/DemoAugmentedRL.md",
    "summary": "Addresses scalable dexterity in robotics. Introduces mimic-one, a training recipe combining imitation learning with reinforcement signals to refine motor skills. Demonstrates robust transfer of complex manipulation abilities across varied robot platforms."
  },
  {
    "title": "Prioritised Level Replay",
    "date_read": "2025-06-18",
    "link": "open_endedness_and_auto_curriculums/PrioritisedLevelReplay.md",
    "summary": "Targets efficient curriculum learning. Proposes prioritized level replay, selecting training scenarios based on their estimated learning impact. Demonstrates accelerated skill acquisition and stronger generalization by focusing on the most informative levels."
  },
  {
    "title": "Self-Adapting Language Models",
    "date_read": "2025-06-19",
    "link": "LLM_reinforcement_learning/SelfAdaptingLanguageModels.md",
    "summary": "Addresses static performance in pre-trained language systems. Introduces a self-adaptation framework enabling models to adjust internal parameters based on real-time feedback. Demonstrates sustained performance improvements and flexibility across shifting input distributions."
  },
  {
    "title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning",
    "date_read": "2025-06-20",
    "link": "LLM_reinforcement_learning/LLMsForStrategicPlanning.md",
    "summary": "Targets long-term strategic planning adaptability. Presents self-evolving agent architectures that iteratively modify strategies using performance feedback. Demonstrates superior planning capabilities and dynamic policy refinement in complex decision environments."
  },
  {
    "title": "KINETIX: INVESTIGATING THE TRAINING OF GENERAL AGENTS THROUGH OPEN-ENDED PHYSICS-BASED CONTROL TASKS",
    "date_read": "2025-06-21",
    "link": "distribution_and_gpu_acceleration/KInetixGeneralRL.md",
    "summary": "Explores training general-purpose agents via physics simulations. Introduces KINETIX, a framework generating open-ended control challenges in simulated environments. Demonstrates agents capable of diverse skill acquisition and robust transfer across varied tasks."
  },
  {
    "title": "Superintelligence From First Principles (blog post)",
    "date_read": "2025-06-22",
    "link": "open_endedness_and_auto_curriculums/SuperintelligenceFromFirstPrinciples.md",
    "summary": "Examines theoretical foundations for superintelligent systems. Derives core principles of intelligence dynamics through first-principles analysis. Argues that open-ended learning and iterative self-improvement drive emergent superintelligence."
  },
  {
    "title": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning",
    "date_read": "2025-06-23",
    "link": "open_endedness_and_auto_curriculums/AutoCurriculumAutonomousDriving.md",
    "summary": "Addresses curriculum adaptation in autonomous driving. Proposes automatic curriculum learning that adjusts scenario difficulty based on agent performance. Demonstrates faster, more robust policy learning in diverse simulated traffic environments."
  },
  {
    "title": "How Visual Representations Map to Language Feature Space in Multimodal LLMs",
    "date_read": "2025-06-24",
    "link": "general_training/SharedRepresentationsInVLMs.md",
    "summary": "Investigates alignment between image and text embeddings in multimodal models. Analyzes how visual features project into language spaces. Proposes mapping techniques to enhance cross-modal retrieval accuracy and reasoning consistency."
  },
  {
    "title": "Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach",
    "date_read": "2025-06-25",
    "link": "open_endedness_and_auto_curriculums/MultiAgentCurriculumSelfPlay.md",
    "summary": "Targets scalable multi-agent skill development in Pommerman. Combines curriculum learning with population-based self-play to evolve strategies. Demonstrates accelerated strategic adaptation and emergent cooperation among agents."
  },
  {
    "title": "Automatic Curriculum Design for Zero-Shot Human AI Coordination",
    "date_read": "2025-06-26",
    "link": "open_endedness_and_auto_curriculums/AutoCurriculumForHumanAICoordination.md",
    "summary": "Addresses AI-human collaboration without prior joint training. Proposes automatic curriculum design that scaffolds tasks to foster coordination. Demonstrates improved zero-shot collaboration and mutual adaptability in human-AI teams."
  },
  {
    "title": "OMNI-EPIC: Open-Endedness Via Models of Human Notions of Interestingness With Environments Programmed In Code",
    "date_read": "2025-06-28",
    "link": "open_endedness_and_auto_curriculums/OpenEndednessUsingLLMS.md",
    "summary": "Explores encoding human curiosity into environment generation. Introduces OMNI-EPIC, which models human-defined interest metrics to program dynamic challenges. Demonstrates richer open-ended exploration aligned with human preferences."
  },
  {
    "title": "Self-Supervised Video Models Enable Understanding, Prediction and Planning (V-JEPA)",
    "date_read": "2025-06-30",
    "link": "general_training/V-JEPA2.md",
    "summary": "Addresses dependency on labeled video data. Introduces V-JEPA, a self-supervised framework learning representations via masked frame prediction. Demonstrates improved video understanding, prediction accuracy, and planning capabilities without annotations."
  },
  {
    "title": "Open-Endedness is Essential for Artificial Superhuman Intelligence",
    "date_read": "2025-07-01",
    "link": "open_endedness_and_auto_curriculums/SuperintelligenceFromFirstPrinciples.md",
    "summary": "Argues continuous innovation is critical for superhuman AI. Analyzes theoretical bases of open-ended learning and self-improvement. Demonstrates that systems embracing perpetual novelty outperform static architectures in complex domains."
  },
  {
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "date_read": "2025-07-02",
    "link": "LLM_reinforcement_learning/SelfPlayZeroSumGames.md",
    "summary": "Targets enhanced reasoning through competitive play. Introduces SPIRAL, a multi-turn self-play framework on zero-sum games that refines strategies via reinforcement signals. Demonstrates deeper inference skills and superior adversarial task performance."
  },
  {
    "title": "Training extremely large neural networks across thousands of GPUs by Jeremy Jordan",
    "date_read": "2025-07-04",
    "link": "distribution_and_gpu_acceleration/TrainingOnThousandsOfGPUs.md",
    "summary": "Addresses scalability of massive model training. Presents system designs and optimization strategies for distributing workloads across thousands of GPUs. Demonstrates significant throughput gains and reduced training times on large-scale benchmarks."
  },
  {
    "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
    "date_read": "2025-07-05",
    "link": "distribution_and_gpu_acceleration/IMPALA_DistributedRL.md",
    "summary": "Addresses stability and efficiency in distributed RL. Introduces an importance-weighted actor-learner architecture enabling scalable parallel training. Demonstrates improved learning efficiency and stability on large-scale reinforcement learning tasks."
  },
  {
    "title": "TorchBeast: A PyTorch Platform for Distributed RL",
    "date_read": "2025-07-05",
    "link": "distribution_and_gpu_acceleration/TorchBeastDistributedPyTorch.md",
    "summary": "Addresses need for unified distributed RL frameworks. Presents TorchBeast, a PyTorch-based system streamlining large-scale agent training. Demonstrates efficient resource utilization and faster iteration on benchmark reinforcement learning challenges."
  },
  {
    "title": "Distributed PPO Blog Post",
    "date_read": "2025-07-07",
    "link": "distribution_and_gpu_acceleration/DistributedPPO.md",
    "summary": "Discusses scaling proximal policy optimization across distributed systems. Details architectural adaptations for efficient parallel updates. Demonstrates substantial throughput improvements and reduced convergence times in policy optimization workloads."
  },
  {
    "title": "Reinforcement Learning with Docker",
    "date_read": "2025-07-08",
    "link": "distribution_and_gpu_acceleration/DockerInRL.md",
    "summary": "Addresses reproducibility and environment consistency in RL experiments. Introduces Docker-based workflows encapsulating dependencies and environments. Demonstrates streamlined deployment and consistent results across varied hardware setups."
  },
  {
    "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning",
    "date_read": "2025-07-09",
    "link": "finance_applications/FinCoT.md",
    "summary": "Targets interpretability in financial decision-making. Proposes FinCoT, integrating chain-of-thought traces grounded in expert analysis. Demonstrates improved forecasting accuracy and transparent reasoning in investment strategy tasks."
  },
  {
    "title": "Illuminating search spaces by mapping elites",
    "date_read": "2025-07-10",
    "link": "open_endedness_and_auto_curriculums/MAP_Elites.md",
    "summary": "Addresses opaque optimization dynamics. Introduces an elite-mapping technique that visualizes high-performing solution regions. Demonstrates enhanced insight into algorithm behavior and guides improved parameter tuning for diverse optimization challenges."
  },
  {
    "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models",
    "date_read": "2025-07-11",
    "link": "open_endedness_and_auto_curriculums/FoundationModelSelfPlay.md",
    "summary": "Targets automated strategy generation. Uses self-play with foundation models to invent and refine novel tactics. Demonstrates emergent complex strategies and continuous innovation without manual curriculum design."
  },
  {
    "title": "How to scale RL to 10^26 FLOPs blog by Jack Morris",
    "date_read": "2025-07-12",
    "link": "LLM_reinforcement_learning/ScalingRLto10^26FLOPS.md",
    "summary": "Addresses extreme compute scaling for reinforcement learning. Explores hardware and software optimizations to enable workloads at 10^26 floating-point operations. Demonstrates feasibility of massive-scale training with reduced overhead on specialized platforms."
  },
  {
    "title": "INTELLECT-1 Technical Report",
    "date_read": "2025-07-14",
    "link": "https://arxiv.org/html/2412.01152v1",
    "summary": "Addresses centralized training limitations by enabling global, decentralized training of large language models. Introduces the Prime framework with ElasticDeviceMesh, hybrid FSDP-DiLoCo, and int8 ring-all-reduce for fault-tolerant distributed training. Demonstrates training a 10B-parameter model on 1T tokens across 14 heterogeneous nodes with >83% utilization."
  },
  {
    "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
    "date_read": "2025-07-14",
    "link": "https://arxiv.org/abs/2203.05482",
    "summary": "Addresses inefficiencies in hyperparameter sweeps where only a single fine-tuned model is selected. Introduces “model soups,” averaging weights from multiple fine-tuned models into one without extra inference cost. Demonstrates improved accuracy and robustness on ImageNet and diverse downstream benchmarks."
  },
  {
    "title": "Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning",
    "date_read": "2025-07-15",
    "link": "https://arxiv.org/abs/2401.08632",
    "summary": "Addresses inefficiencies of evolutionary quality-diversity methods in high-dimensional spaces due to random mutations. Introduces DCRL-MAP-Elites, leveraging a descriptor-conditioned reinforcement learning actor as a generative model to propose diverse solutions each generation. Demonstrates improved solution quality, diversity, and descriptor reproducibility over prior approaches."
  },
  {
    "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models",
    "date_read": "2025-07-16",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/general_training/DoFoundationModelsLearnWorldModels.md",
    "summary": "Assessing whether large prediction models internalize underlying systems instead of surface patterns remains challenging. This study designs an inductive bias probe by adapting models to synthetic data from known world dynamics. It reveals models learn task-specific heuristics but fail to adopt true generative laws, hindering generalization."
  },
  {
    "title": "Deep Dive into Yann LeCun’s JEPA by Rohit Bandaru",
    "date_read": "2025-07-17",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/open_endedness_and_auto_curriculums/DeepDiveIntoYannLecunsJEPA.md",
    "summary": "Large language models lack planning and common-sense inference due to token-based generation constraints. This analysis examines JEPA, an energy-based joint embedding predictive architecture combining hierarchical modules to predict latent future representations. It highlights JEPA’s capacity to generate plausible future states, offering a route to improve reasoning and planning capabilities."
  },
  {
    "title": "All AI Models Might Be The Same by Jack Morris",
    "date_read": "2025-07-18",
    "link": "https://blog.jxmo.io/p/there-is-only-one-model",
    "summary": "Investigates whether differences between AI models reflect distinct solutions or converge to a single optimal compression. It frames learning as data compression via information theory. Shows that models converge toward a unique representation, explaining why varied architectures yield similar performance."
  },
  {
    "title": "Multi-Agent Diagnostics for Robustness via Illuminated  (MADRID)",
    "date_read": "2025-07-19",
    "link": "https://arxiv.org/abs/2401.13460",
    "summary": "Addresses robustness gaps in multi-agent reinforcement learning by adversarially searching for failure scenarios. Applies MAP-Elites over ball positions in a simulated football environment, using regret against reference policies as fitness. Reveals diverse weaknesses in trained policies across the pitch."
  },
  {
    "title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL",
    "date_read": "2025-07-20",
    "link": "https://arxiv.org/abs/2409.12798",
    "summary": "Addresses sparse and delayed reward assignment in reinforcement learning by using a language model to decompose tasks into subgoals and shape rewards. The proposed approach assigns auxiliary rewards when subgoals are met via zero-shot model evaluation. Experiments on MiniHack show effective credit assignment without additional training."
  },
  {
    "title": "TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play",
    "date_read": "2025-07-20",
    "link": "https://arxiv.org/abs/2302.07515",
    "summary": "Tackles the challenge of learning coordinated strategies in complex multi-agent environments by combining curriculum learning and self-play. The approach gradually increases scenario difficulty while teams train against evolving opponents. Evaluations on a simulated football platform show a 30%+ boost in win rates and robustness across varied game settings."
  },
  {
    "title": "Model-Based Meta Automatic Curriculum Learning",
    "date_read": "2025-07-23",
    "link": "https://openreview.net/pdf?id=Kp716SJ5dbJ",
    "summary": "Addresses how to generate effective sequences of intermediate tasks to accelerate learning across varied target tasks. Introduces MM-ACL, which learns a model predicting performance gains between tasks and greedily selects curricula. Shows improved sample efficiency in a navigation benchmark compared to random and bandit baselines."
  },
  {
    "title": "Eurekaverse: Environment Curriculum Generation via Large Language Models",
    "date_read": "2025-07-23",
    "link": "https://arxiv.org/abs/2411.01775",
    "summary": "Addresses the challenge of designing environment curricula for open-ended agent training. Proposes Eurekaverse, which uses a language model to propose successive environment modifications based on agent feedback. Experiments in grid-world tasks show accelerated skill acquisition and increased behavioral diversity compared to manual or random curricula."
  },
   {
    "title": "Benchmarking Population-Based Reinforcement Learning across Robotic Tasks with GPU-Accelerated Simulation",
    "date_read": "2025-07-25",
    "link": "distribution_and_gpu_acceleration/PopulationHyperParameterSearch.md",
    "summary": "Population-based reinforcement learning methods often require extensive simulation time for robotic tasks. This work introduces a GPU-accelerated simulation framework to benchmark and tune diverse agent populations efficiently. Experiments across multiple robotic environments demonstrate substantial speedups and performance gains compared to CPU-based baselines."
  },
  {
    "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning",
    "date_read": "2025-07-26",
    "link": "open_endedness_and_auto_curriculums/DiscoverAutoCurriculaForSparseRewards.md",
    "summary": "Sparse-feedback learning agents struggle to solve tasks where rewards are rare. This work proposes an automated curriculum algorithm that dynamically adjusts task difficulty based on agent progress. Tests on challenging benchmarks show faster learning and higher success rates than fixed or manual curricula."
  },
  {
    "title": "Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization",
    "date_read": "2025-07-27",
    "link": "general_training/WinnerTakesItAllPopulationBasedRL.md",
    "summary": "Solving combinatorial design problems with single policies limits solution diversity. This work develops a population training scheme that rewards only the best-performing agent per instance, causing implicit specialization. Results on multiple combinatorial benchmarks show improved solution quality and inference efficiency over existing methods."
  },
  {
    "title": "Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation",
    "date_read": "2025-07-28",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/marl/StandardisingOfflineMARLResearch.md",
    "summary": "The paper addresses misleading progress in offline multi‑agent reinforcement learning by proposing consistent baseline implementations and unified evaluation protocols. Experiments using these standards reveal that many recent methods underperform, underscoring the necessity of rigorous and reproducible benchmarking."
  },
  {
    "title": "Communicative Reinforcement Learning Agents for Landmark Detection in Brain Images",
    "date_read": "2025-07-29",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/marl/MARLForBrainImaging.md",
    "summary": "This work tackles efficient anatomical landmark detection in brain scans by designing reinforcement learning agents that exchange compact messages. Through collaborative communication, these agents achieve more accurate and faster landmark localization compared to independent approaches, demonstrating the value of inter‑agent messaging."
  },
  {
    "title": "Intelligent Railway Capacity and Traffic Management Using Multi-Agent Deep Reinforcement Learning",
    "date_read": "2025-07-30",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/marl/TrainSchedulingWithMARL.md",
    "summary": "The study tackles scheduling and dispatching challenges in congested rail networks by deploying multi‑agent deep reinforcement learning within a detailed train simulator. The learned policies significantly boost on‑time performance and rapidly adapt to disruptions, offering a scalable approach for automated traffic management."
  },
    {
    "title": "Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning",
    "date_read": "2025-07-31",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/non_LLM_reinforcement_learning/Gym4ReaL.md",
    "summary": "Reinforcement learning benchmarks often ignore real-world complexities like non-stationarity and partial observability. The authors present Gym4ReaL, a suite of diverse environments simulating practical constraints. Experiments show standard RL methods remain competitive against rule-based baselines, underscoring the need for advanced algorithms."
  },
  {
    "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
    "date_read": "2025-08-01",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/LLMs/GEPAPromptEngineering.md",
    "summary": "Optimizing complex prompts for large language models is inefficient with traditional reinforcement learning due to costly rollouts and noisy feedback. The authors introduce GEPA, combining natural language reflection and genetic-Pareto search to evolve prompts iteratively. GEPA achieves 10–20% better performance with up to 35× fewer rollouts."
  },
  {
    "title": "Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels",
    "date_read": "2025-08-02",
    "link": "https://github.com/domrigby/domrigby.github.io/blob/main/non_LLM_reinforcement_learning/FORECASTER_TimeAbstractedPlanning.md",
    "summary": "Planning in high-dimensional environments with sparse rewards often requires long-horizon reasoning. The authors propose Forecaster, a hierarchical method that learns temporally abstract world models and uses tree-search to select high-level goals, alongside a low-level policy for execution. It improves sample efficiency and generalizes in AntMaze tasks."
  },
    {
    "title": "Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning",
    "date_read": "2025-07-31",
    "link": "non_LLM_reinforcement_learning/Gym4Real.md",
    "summary": "Addresses the lack of standardized testing for real-world reinforcement learning. Introduces a suite of benchmark environments emulating practical constraints like delays and noise. Demonstrates its use for comparing algorithms, revealing performance gaps between simulated and realistic conditions."
  },
  {
    "title": "Hierarchical Reasoning Model",
    "date_read": "2025-08-04",
    "link": "general_training/HierarchicalReasoningModel.md",
    "summary": "Tackles the challenge of multi-step reasoning over complex inputs. Proposes a model that decomposes reasoning into hierarchical levels, each handling progressively abstract tasks. Shows improved accuracy and interpretability on reasoning benchmarks compared to flat, single-level approaches."
  },
  {
    "title": "GOAL: A Generalist Combinatorial Optimization Agent Learner",
    "date_read": "2025-08-05",
    "link": "non_LLM_reinforcement_learning/GOAL_general_transformer_for_CO.md",
    "summary": "Targets generalization across combinatorial optimization problems. Develops a transformer-based agent trained on diverse problem instances with reinforcement learning. Finds the model can adapt to unseen tasks with minimal fine-tuning, outperforming specialized solvers in cross-domain tests."
  },
  {
    "title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
    "date_read": "2025-08-06",
    "link": "architectures/SurveyOfGraphTransformers.md",
    "summary": "Examines the integration of transformer mechanisms into graph processing. Reviews architectures, theoretical foundations, and application domains. Concludes that graph transformers improve scalability and expressiveness over traditional methods, but face open challenges in efficiency and handling large-scale graphs."
  },
  {
    "title": "A Gentle Introduction to Graph Neural Networks",
    "date_read": "2025-08-07",
    "link": "architectures/IntroToGraphNNs.md",
    "summary": "Introduces the basics of learning on graph-structured data. Explains message passing, node and edge features, and common architectures. Demonstrates how GNNs capture structural relationships, enabling effective predictions in tasks like classification and link prediction."
  },
  {
    "title": "Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation",
    "date_read": "2025-08-08",
    "link": "non_LLM_reinforcement_learning/GraphTransformersForControllingSwarms.md",
    "summary": "Addresses coordination in multi-agent systems with complex interactions. Proposes combining graph neural networks and transformers to model dynamic relationships. Shows improved cooperation and scalability in simulated swarm control tasks compared to conventional message-passing approaches."
  },
  {
    "title": "Open-Ended Learning Leads to Generally Capable Agents",
    "date_read": "2025-08-09",
    "link": "open_endedness_and_auto_curriculums/OpenEndedLearningLeadstoGenerallyCapableAgents.md",
    "summary": "Seeks to create agents that continually acquire new skills. Uses a procedurally generated multi-agent world with evolving tasks and open-ended training. Finds agents develop general capabilities, zero-shot transfer, and emergent behaviors like tool use and cooperation."
  },
  {
    "title": "Sable: a Performant, Efficient and Scalable Sequence Model for MARL",
    "date_read": "2025-08-10",
    "link": "marl/SabelMATButWithRetention.md",
    "summary": "The paper addresses the challenge of handling long multi-agent interaction sequences efficiently. It introduces a lightweight recurrent-style sequence model that processes data in chunks rather than full histories. Results show it scales better than transformers while preserving accuracy in multi-agent reinforcement learning tasks."
  },
  {
    "title": "SMX: Sequential Monte Carlo Planning for Expert Iteration",
    "date_read": "2025-08-11",
    "link": "non_LLM_reinforcement_learning/model_based_methods/SMX_ParticleFilterPolicyImprovement.md",
    "summary": "This work tackles the problem of planning under uncertainty when training decision-making agents. It proposes a particle-based simulation method to guide expert iteration, efficiently approximating future outcomes. The approach yields more stable policies than standard planning methods, improving performance across challenging control environments."
  },
  {
    "title": "ProRL V2 - Prolonged Training Validates RL Scaling Laws",
    "date_read": "2025-08-13",
    "link": "LLM_reinforcement_learning/ProRL2.md",
    "summary": "The study investigates whether reinforcement learning benefits from extended training beyond common horizons. Using larger models trained for longer durations, it systematically evaluates performance scaling. Findings confirm consistent improvement with scale, supporting the existence of reinforcement learning scaling laws similar to those in language models."
  },
  {
    "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning",
    "date_read": "2025-08-16",
    "link": "distribution_and_gpu_acceleration/IsaacGym.md",
    "summary": "This paper tackles the challenge of slow physics simulation in robot learning. It introduces a GPU-based simulator that enables batched environments with direct memory access from training frameworks. The method drastically reduces data transfer overhead, enabling orders-of-magnitude faster training for reinforcement learning agents."
  },
  {
    "title": "The 37 Implementation Details of Proximal Policy Optimization",
    "date_read": "2025-08-18",
    "link": "non_LLM_reinforcement_learning/37PPOImplementationDetails.md",
    "summary": "The paper addresses performance inconsistencies in Proximal Policy Optimization implementations. It systematically identifies and tests 37 implementation details across environments. Findings show that minor code-level choices, such as normalization and clipping, significantly impact stability and learning speed, emphasizing the importance of careful engineering."
  },
  {
    "title": "Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning",
    "date_read": "2025-08-21",
    "link": "distribution_and_gpu_acceleration/LearningToWalkInMinutes.md",
    "summary": "This work addresses the long training time of locomotion agents. It uses hundreds of GPUs to train agents in parallel across thousands of environments. The method achieves humanoid walking in under 10 minutes, showing that large-scale parallelism can replace algorithmic tricks for fast learning."
  },
  {
    "title": "Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space",
    "date_read": "2025-08-22",
    "link": "non_LLM_reinforcement_learning/HybridACRLWithParameterisedActionSpace.md",
    "summary": "This paper deals with decision-making tasks involving both discrete choices and continuous parameters. It proposes a hybrid actor-critic framework that jointly learns policies over both spaces. The approach outperforms baselines on control tasks, showing better handling of structured action representations."
  },
  {
    "title": "DD-PPO: LEARNING NEAR-PERFECT POINTGOAL NAVIGATORS FROM 2.5 BILLION FRAMES",
    "date_read": "2025-08-23",
    "link": "distribution_and_gpu_acceleration/DD-PPO.md",
    "summary": "This paper tackles scalable embodied navigation. It introduces a distributed reinforcement learning system that combines synchronous data collection and policy updates across multiple machines. The approach trains agents to near-perfect navigation ability using billions of frames, demonstrating strong generalization in real-world-like 3D environments."
  },
  {
    "title": "The Bitter Lesson",
    "date_read": "2025-08-26",
    "link": "non_LLM_reinforcement_learning/BitterLesson.md",
    "summary": "This essay reflects on the history of AI research, arguing that human-designed solutions are consistently outperformed by those that scale with computation and data. It advocates for general-purpose methods that leverage compute rather than handcrafted models, highlighting a recurring lesson across decades."
  },
  {
    "title": "In-Context Reinforcement Learning for Variable Action Spaces",
    "date_read": "2025-09-01",
    "link": "non_LLM_reinforcement_learning/HeadlessADInContextRL.md",
    "summary": "The paper addresses decision-making with changing action sets. It proposes a transformer-based architecture that conditions on past experiences to adapt to variable action spaces. The method enables few-shot adaptation across tasks and shows promising generalization without retraining in multi-agent and combinatorial settings."
  },
  {
    "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX",
    "date_read": "2025-09-03",
    "link": "distribution_and_gpu_acceleration/JumanjiCombOpJaxEnvs.md",
    "summary": "This paper presents a new benchmark suite of combinatorial and continuous control environments built in JAX. It emphasizes compatibility with vectorized training and gradient-based methods. The environments enable scalable and reproducible research across structured problems like routing, packing, and inventory control."
  },
  {
    "title": "Efficiently Quantifying Individual Agent Importance in Cooperative MARL",
    "date_read": "2025-09-05",
    "link": "marl/AgentImportance.md",
    "summary": "The paper focuses on understanding agent contributions in cooperative multi-agent settings. It introduces an efficient method to estimate each agent’s importance by measuring counterfactual value changes. This approach allows scalable credit assignment and improves learning by identifying which agents most influence outcomes."
  },
    {
    "title": "Maarten Grootendorst's blog post on A Visual Guide to Quantization",
    "date_read": "2025-09-07",
    "link": "general_training/VisualGuideToQuantization.md",
    "summary": "This blog explains the challenge of reducing model size and speed without harming accuracy. It presents quantization methods that map continuous weights to fewer discrete values. The main finding is that careful quantization significantly reduces memory and computation while retaining strong performance."
  },
  {
    "title": "Following Jax MNIST tutorials",
    "date_read": "2025-09-13",
    "link": "distribution_and_gpu_acceleration/LearningJaxNotes.md",
    "summary": "This work addresses the need to learn practical machine learning in JAX. The method involves implementing and training models on the MNIST dataset using JAX primitives. The main finding is that JAX enables concise, efficient code with strong performance and clear insight into numerical computation."
  },
  {
    "title": "Compiling machine learning programs via high-level tracing",
    "date_read": "2025-09-14",
    "link": "distribution_and_gpu_acceleration/JaxSummary.md",
    "summary": "This paper tackles the problem of efficiently running high-level machine learning code on hardware accelerators. It introduces a system that traces Python functions to build optimizable intermediate representations. The main finding is that tracing-based compilation achieves both user-friendly programming and hardware-level execution speed."
  },
  {
    "title": "How to think about GPUs by Google DeepMind",
    "date_read": "2025-09-15",
    "link": "distribution_and_gpu_acceleration/HowToThinkAboutGPUs.md",
    "summary": "This blog addresses the difficulty of understanding GPU architecture for machine learning. It explains how cores, memory, and scheduling interact to enable parallelism. The main finding is that viewing GPUs as throughput-oriented, highly parallel systems clarifies how to design and optimize algorithms for them."
  },
  {
    "title": "RL’S RAZOR: WHY ONLINE REINFORCEMENT LEARNING FORGETS LESS",
    "date_read": "2025-09-16",
    "link": "LLM_reinforcement_learning/WhyOnlineRLRemembersBetter.md",
    "summary": "The paper examines why reinforcement learning systems trained online retain knowledge better than offline ones. It analyses how continuous updates reduce catastrophic forgetting. The main finding is that online training naturally rebalances exposure, leading to more stable memory preservation compared to fixed offline datasets."
  },
  {
    "title": "PyTorch Internals by Edward Wang",
    "date_read": "2025-09-21",
    "link": "distribution_and_gpu_acceleration/PyTorchInternals.md",
    "summary": "This post investigates the internal mechanics of PyTorch, focusing on execution flow and memory handling. It explains how dynamic graphs, autograd, and backend kernels are structured. The key insight is that PyTorch balances flexibility and efficiency through layered abstractions that manage computation and hardware operations effectively."
  },
  {
    "title": "What is Torch Compile?",
    "date_read": "2025-09-22",
    "link": "distribution_and_gpu_acceleration/TorchCompileExplained.md",
    "summary": "The article describes the challenges of speeding up PyTorch models while maintaining flexibility. It introduces Torch Compile, which captures computation graphs and applies optimisations like operator fusion. The main finding is that this approach delivers significant runtime acceleration without requiring developers to alter model definitions."
  },
  {
    "title": "JIT Compilation in JAX",
    "date_read": "2025-09-23",
    "link": "distribution_and_gpu_acceleration/JaxJITCompile.md",
    "summary": "This piece explores how just-in-time compilation improves execution in JAX. It explains graph tracing, operator fusion, and limitations with dynamic control flow. The core finding is that JAX achieves large performance gains by compiling static segments while requiring careful structuring of code to fit compilation constraints."
  },
  {
    "title": "CUDA Study Log 4: Optimizing Constrained Decoding with Triton Kernel",
    "date_read": "2025-09-23",
    "link": "distribution_and_gpu_acceleration/LevelsOfOptimisationForConstrainedDecoding.md",
    "summary": "The blog investigates constrained sequence decoding on GPUs. It implements a custom Triton kernel to reduce overhead from branching and memory inefficiencies. The main outcome is that tailored kernels substantially speed up decoding tasks by exploiting fine-grained GPU control, outperforming generic high-level implementations."
  },
  {
    "title": "Accelerating PyTorch with CUDA Graphs",
    "date_read": "2025-09-24",
    "link": "non_LLM_reinforcement_learning/CUDAGraphsInPyTorch.md",
    "summary": "The article addresses GPU overhead from repeated kernel launches in PyTorch workloads. It applies CUDA Graphs to capture and replay execution patterns efficiently. The key result is that using CUDA Graphs eliminates launch bottlenecks, producing large speedups in stable workloads where graph structures remain consistent."
  },
  {
    "title": "PyTorch Performance Tuning Guide",
    "date_read": "2025-09-26",
    "link": "distribution_and_gpu_acceleration/PyTorchPerformanceAdvice.md",
    "summary": "The guide outlines common performance bottlenecks when training PyTorch models. It recommends practices such as preallocating tensors, optimising data pipelines, and leveraging mixed precision. The main message is that systematic profiling and small adjustments collectively yield major improvements in throughput and memory efficiency."
  },
  {
    "title": "Nvidia Docs: GPU Performance Fundamentals",
    "date_read": "2025-09-27",
    "link": "distribution_and_gpu_acceleration/NvidiaDocsGPUFundamentals.md",
    "summary": "This documentation explains how GPU hardware executes parallel workloads. It describes warps, streaming multiprocessors, and memory hierarchies. The core insight is that effective GPU usage requires minimising divergence and memory latency, enabling schedulers to overlap operations and achieve near-linear scaling with more threads."
  },
  {
    "title": "Nvidia Docs: Optimising Memory Limited Layers",
    "date_read": "2025-09-27",
    "link": "distribution_and_gpu_acceleration/NvidiaDocsMemLimitedLayers.md",
    "summary": "The document addresses neural network layers constrained by memory bandwidth rather than compute. It analyses common cases like embedding lookups and normalisation. The key recommendation is to restructure memory access patterns and reduce data movement, which can significantly increase throughput without altering model accuracy."
  },
  {
    "title": "Nvidia Docs: Matrix Multiplication and Quantisation Background",
    "date_read": "2025-09-28",
    "link": "distribution_and_gpu_acceleration/NvidiaMatMulAndQuantisation.md",
    "summary": "This resource explains the mathematics and implementation of matrix multiplication and quantisation on GPUs. It highlights how data layout, precision reduction, and tensor cores influence performance. The finding is that careful use of lower precision arithmetic yields large efficiency gains while maintaining sufficient accuracy for training."
  },
  {
    "title": "On the Design of KL-Regularised Policy Gradient Algorithms for LLM Reasoning",
    "date_read": "2025-09-30",
    "link": "general_training/KLDivergenceRegularisation.md",
    "summary": "The paper studies how to stabilise reinforcement learning for reasoning tasks in large models. It designs policy gradient methods with KL regularisation to balance exploration and constraint. The main conclusion is that KL control provides more reliable optimisation, reducing collapse while guiding models toward useful behaviours."
  },
  {
    "title": "Current Best Practices for Training LLMs from Scratch",
    "date_read": "2025-10-01",
    "link": "general_training/wandbTrainingTFFromScratch.md",
    "summary": "The article compiles guidelines for training large language models efficiently. It covers data quality, scaling laws, hyperparameter tuning, and monitoring practices. The main finding is that disciplined attention to pipeline design and resource allocation enables stable large-scale training, reducing failures and wasted compute during development."
  },
    {
    "title": "Polychromic Objectives for Reinforcement Learning",
    "date_read": "2025-10-02",
    "link": "non_LLM_reinforcement_learning/DiversePoliciesThroughSetRL.md",
    "summary": "The paper tackles the challenge of learning diverse policies within reinforcement learning. It introduces polychromic objectives that treat policy optimization as a set-valued problem. The main finding is that this approach encourages diversity and robustness by explicitly optimising multiple complementary behaviours rather than a single optimal policy."
  },
  {
    "title": "RL Grokking Recipe- How Can We Enable LLMs to Solve Previously Unsolvable Tasks",
    "date_read": "2025-10-03",
    "link": "non_LLM_reinforcement_learning/RlForUnsolvableTasks.md",
    "summary": "The article investigates why large models fail on complex reasoning tasks despite scale. It proposes reinforcement learning fine-tuning recipes that reward intermediate reasoning steps. The key result is that targeted reward shaping and curriculum design can unlock problem-solving capabilities previously unreachable through standard supervised or unsupervised training."
  },
  {
    "title": "Debugging RL, Without the Agonizing Pain",
    "date_read": "2025-10-04",
    "link": "general_training/DebugginRLWithoutThePain.md",
    "summary": "This blog post addresses the difficulty of diagnosing reinforcement learning failures. It outlines systematic debugging methods using controlled experiments, metric tracking, and ablation studies. The main conclusion is that structured diagnostic practices dramatically reduce wasted training time by identifying environment, reward, or policy instability early in development."
  },
  {
    "title": "Flash Attention Blog Post",
    "date_read": "2025-10-06",
    "link": "distribution_and_gpu_acceleration/FlashAttention.md",
    "summary": "The post explains the performance issues of standard attention mechanisms caused by memory-bound operations. It introduces FlashAttention, an algorithm that computes attention in tiled, IO-efficient segments. The main finding is that this method greatly improves throughput and memory efficiency while producing mathematically equivalent results to conventional attention."
  }
]

