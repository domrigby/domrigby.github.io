<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script>                <div id="f3e9380a-cda5-484c-85f6-cc4829f304e5" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("f3e9380a-cda5-484c-85f6-cc4829f304e5")) {                    Plotly.newPlot(                        "f3e9380a-cda5-484c-85f6-cc4829f304e5",                        [{"customdata":[["Absolute Zero Reasoner","Reasoning & LLM-RL"],["Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment","Reasoning & LLM-RL"],["ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models","Reasoning & LLM-RL"],["JaxMARL: Multi-Agent RL Environments and Algorithms in JAX","Reasoning & LLM-RL"],["Illusion of Thinking: Understanding Strengths and Limitations of Large Reasoning Models (LRMs)","Reasoning & LLM-RL"],["Beyond the 80\u002f20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning","Reasoning & LLM-RL"],["Play to Generalize: Learning to Reason Through Game Play","Reasoning & LLM-RL"],["Self-Adapting Language Models","Reasoning & LLM-RL"],["How Visual Representations Map to Language Feature Space in Multimodal LLMs","Reasoning & LLM-RL"],["SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning","Reasoning & LLM-RL"],["Reinforcement Learning with Docker","Reasoning & LLM-RL"],["INTELLECT-1 Technical Report","Reasoning & LLM-RL"],["Deep Dive into Yann LeCun\u2019s JEPA by Rohit Bandaru","Reasoning & LLM-RL"],["GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning","Reasoning & LLM-RL"],["Hierarchical Reasoning Model","Reasoning & LLM-RL"],["Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX","Reasoning & LLM-RL"],["Efficiently Quantifying Individual Agent Importance in Cooperative MARL","Reasoning & LLM-RL"],["Following Jax MNIST tutorials","Reasoning & LLM-RL"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Reasoning & LLM-RL","marker":{"color":"#636EFA","symbol":"circle"},"mode":"markers","name":"Reasoning & LLM-RL","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"M4iVwGniSMDz6KbAE3EwwWl9s8AM3bjAW6EqwGlSIsFWOvLAvYAKwJ\u002f7EcHpzynBmY\u002fnwOGU4sBT4njApjxAwQK+KMCQCDrB"},"xaxis":"x","y":{"dtype":"f4","bdata":"7Bi7v\u002fNIcj+T1fm\u002fFS+swMOSoD4rxZe\u002fJSg+wOogVb9eAsw\u002fCgVNwFJS1cC1sh6+wPHYvkEK978IqXG\u002flUmkwNTK5D8DtZLA"},"yaxis":"y","type":"scatter"},{"customdata":[["\u03c0_{0.5}: a Vision-Language-Action Model with Open-World Generalization","Control & Planning Models"],["TD-MPC2: Scalable, Robust World Models for Continuous Control","Control & Planning Models"],["Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (Demo Augmented RL)","Control & Planning Models"],["Spurious Rewards: Rethinking Training Signals in RLVR","Control & Planning Models"],["Reinforcement Pre-Training","Control & Planning Models"],["Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards","Control & Planning Models"],["mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity","Control & Planning Models"],["Self-Supervised Video Models Enable Understanding, Prediction and Planning (V-JEPA)","Control & Planning Models"],["Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL","Control & Planning Models"],["Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels","Control & Planning Models"],["SMX: Sequential Monte Carlo Planning for Expert Iteration","Control & Planning Models"],["Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space","Control & Planning Models"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Control & Planning Models","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"Control & Planning Models","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"UNzxQNuoH0CtNANBI0edQO0e1T+qBIZAYHkDQdDTS0A8Em1AkTAIQPDQBD\u002fcvGdA"},"xaxis":"x","y":{"dtype":"f4","bdata":"9krbQJOmhkDTX6VAson2QOb1l0DZ6tVAfhqWQDGgK0GYtfRAALi3QFXckkDr17VA"},"yaxis":"y","type":"scatter"},{"customdata":[["JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Age Reinforcement Learning","PPO & RL Methods"],["Distributed PPO Blog Post","PPO & RL Methods"],["FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning","PPO & RL Methods"],["Illuminating search spaces by mapping elites","PPO & RL Methods"],["Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation","PPO & RL Methods"],["The 37 Implementation Details of Proximal Policy Optimization","PPO & RL Methods"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"PPO & RL Methods","marker":{"color":"#00CC96","symbol":"circle"},"mode":"markers","name":"PPO & RL Methods","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"15spwMEOAsB6Z7W\u002f5DKAwPk5P8BxQR\u002fA"},"xaxis":"x","y":{"dtype":"f4","bdata":"\u002fc\u002fiQLddF0F3LX1APsUnQZrC0UD2iRtB"},"yaxis":"y","type":"scatter"},{"customdata":[["Synthetic Experience Replay","Curricula & Scalable RL"],["Ultimate Guide to Supervised Fine-Tuning","Curricula & Scalable RL"],["CHIRPs: Change-Induced Regret Proxy Metrics for Lifelong Reinforcement Learning","Curricula & Scalable RL"],["Prioritised Level Replay","Curricula & Scalable RL"],["KINETIX: INVESTIGATING THE TRAINING OF GENERAL AGENTS THROUGH OPEN-ENDED PHYSICS-BASED CONTROL TASKS","Curricula & Scalable RL"],["Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning","Curricula & Scalable RL"],["IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures","Curricula & Scalable RL"],["TorchBeast: A PyTorch Platform for Distributed RL","Curricula & Scalable RL"],["Multi-Agent Diagnostics for Robustness via Illuminated  (MADRID)","Curricula & Scalable RL"],["Model-Based Meta Automatic Curriculum Learning","Curricula & Scalable RL"],["Eurekaverse: Environment Curriculum Generation via Large Language Models","Curricula & Scalable RL"],["Benchmarking Population-Based Reinforcement Learning across Robotic Tasks with GPU-Accelerated Simulation","Curricula & Scalable RL"],["DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning","Curricula & Scalable RL"],["Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation","Curricula & Scalable RL"],["Communicative Reinforcement Learning Agents for Landmark Detection in Brain Images","Curricula & Scalable RL"],["Intelligent Railway Capacity and Traffic Management Using Multi-Agent Deep Reinforcement Learning","Curricula & Scalable RL"],["Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning","Curricula & Scalable RL"],["Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning","Curricula & Scalable RL"],["Sable: a Performant, Efficient and Scalable Sequence Model for MARL","Curricula & Scalable RL"],["ProRL V2 - Prolonged Training Validates RL Scaling Laws","Curricula & Scalable RL"],["Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning","Curricula & Scalable RL"],["Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning","Curricula & Scalable RL"],["DD-PPO: LEARNING NEAR-PERFECT POINTGOAL NAVIGATORS FROM 2.5 BILLION FRAMES","Curricula & Scalable RL"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Curricula & Scalable RL","marker":{"color":"#AB63FA","symbol":"circle"},"mode":"markers","name":"Curricula & Scalable RL","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"Uu7mQMmmiT5ZvSdASH8SQIeE\u002f0CbO45AiOdvP99bMj\u002fZRp5AAjX8P0h08UAjLAlBncNaQHqen0DjQhtBAfPNQGEwnED3ZqlAHi1EQDzyLEGvvANBI6ocQZzxAUE="},"xaxis":"x","y":{"dtype":"f4","bdata":"cg1FQC+Pp7\u002f+7Cy\u002fAEgtwCQYO8AE2UrAx\u002fzmP6YTxz\u002fSrJ2+DCXMvyNwiMB\u002ffwK\u002fm44cwAhM0D9PZ88\u002fQmgaPxAMNED4NkZA+pD7P3R3fEAvR+2+7RZ5vxwnqz8="},"yaxis":"y","type":"scatter"},{"customdata":[["Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions","Open-Ended RL & Agents"],["Reinforcement Learning Teachers of Test Time Scaling","Open-Ended RL & Agents"],["Agents of Change: Self-Evolving LLM Agents for Strategic Planning","Open-Ended RL & Agents"],["Superintelligence From First Principles (blog post)","Open-Ended RL & Agents"],["Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach","Open-Ended RL & Agents"],["Automatic Curriculum Design for Zero-Shot Human AI Coordination","Open-Ended RL & Agents"],["OMNI-EPIC: Open-Endedness Via Models of Human Notions of Interestingness With Environments Programmed In Code","Open-Ended RL & Agents"],["Open-Endedness is Essential for Artificial Superhuman Intelligence","Open-Ended RL & Agents"],["Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models","Open-Ended RL & Agents"],["Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning","Open-Ended RL & Agents"],["What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models","Open-Ended RL & Agents"],["All AI Models Might Be The Same by Jack Morris","Open-Ended RL & Agents"],["TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play","Open-Ended RL & Agents"],["Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization","Open-Ended RL & Agents"],["GOAL: A Generalist Combinatorial Optimization Agent Learner","Open-Ended RL & Agents"],["Open-Ended Learning Leads to Generally Capable Agents","Open-Ended RL & Agents"],["The Bitter Lesson","Open-Ended RL & Agents"],["In-Context Reinforcement Learning for Variable Action Spaces","Open-Ended RL & Agents"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Open-Ended RL & Agents","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"Open-Ended RL & Agents","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"YptqQBMkuD\u002fearJAMP6FQHBxu0AajOlA\u002fP4eQZljdkB2JJ9AYOHwv5eI8L0s7\u002f0\u002fXBWjQBh8LcD80w3AA37YQKPpPUA3rpC+"},"xaxis":"x","y":{"dtype":"f4","bdata":"l\u002fbTwMq6YcB4jvbAOFohwVpcsMD199XA1FK+wFlFKcFE9tzA054Dwf\u002f4FsGAGTnBcFOOwEAo7MDAz+TA+byfwHfPLcH7SrzA"},"yaxis":"y","type":"scatter"},{"customdata":[["Training extremely large neural networks across thousands of GPUs by Jeremy Jordan","Scaling & Architectures"],["How to scale RL to 10^26 FLOPs blog by Jack Morris","Scaling & Architectures"],["Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time","Scaling & Architectures"],["A Survey of Graph Transformers: Architectures, Theories and Applications","Scaling & Architectures"],["A Gentle Introduction to Graph Neural Networks","Scaling & Architectures"],["Maarten Grootendorst's blog post on A Visual Guide to Quantization","Scaling & Architectures"],["Compiling machine learning programs via high-level tracing","Scaling & Architectures"],["How to think about GPUs by Google DeepMind","Scaling & Architectures"]],"hovertemplate":"\u003cb\u003e%{customdata[0]}\u003c\u002fb\u003e\u003cbr\u003eCluster: %{customdata[1]}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Scaling & Architectures","marker":{"color":"#19D3F3","symbol":"circle"},"mode":"markers","name":"Scaling & Architectures","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"fRQEwajt4MANNPbArFRAwYYhPcFgLAHBVkcBwS+jD8E="},"xaxis":"x","y":{"dtype":"f4","bdata":"Ad3gQAc59kC7QXFAIeCXQKA2iUAA0KhAB98HQSE5+EA="},"yaxis":"y","type":"scatter"},{"fill":"toself","fillcolor":"rgba(99,110,250,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(99,110,250,1)","width":1},"name":"Convex Hulls","showlegend":true,"visible":true,"x":{"dtype":"f4","bdata":"vYAKwAK+KMBWOvLA6c8pwaY8QMGf+xHBvYAKwA=="},"y":{"dtype":"f4","bdata":"CgVNwNTK5D9eAsw\u002ftbIevpVJpMBSUtXACgVNwA=="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(239,85,59,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(239,85,59,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"0NNLQPDQBD\u002fbqB9AYHkDQa00A0FQ3PFA0NNLQA=="},"y":{"dtype":"f4","bdata":"MaArQVXckkCTpoZAfhqWQNNfpUD2SttAMaArQQ=="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(0,204,150,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(0,204,150,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"eme1v8EOAsBxQR\u002fA5DKAwPk5P8B6Z7W\u002f"},"y":{"dtype":"f4","bdata":"dy19QLddF0H2iRtBPsUnQZrC0UB3LX1A"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(171,99,250,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(171,99,250,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"yaaJPkh\u002fEkBIdPFAI6ocQTzyLEH3ZqlAiOdvP99bMj\u002fJpok+"},"y":{"dtype":"f4","bdata":"L4+nvwBILcAjcIjA7RZ5v3R3fED4NkZAx\u002fzmP6YTxz8vj6e\u002f"},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(255,161,90,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(255,161,90,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"EyS4Pxh8LcBg4fC\u002fLO\u002f9P5ljdkD8\u002fh5BXBWjQBMkuD8="},"y":{"dtype":"f4","bdata":"yrphwEAo7MDTngPBgBk5wVlFKcHUUr7AcFOOwMq6YcA="},"type":"scatter"},{"fill":"toself","fillcolor":"rgba(25,211,243,0.15)","hoverinfo":"skip","legendgroup":"hulls","line":{"color":"rgba(25,211,243,1)","width":1},"name":"Convex Hulls","showlegend":false,"visible":true,"x":{"dtype":"f4","bdata":"DTT2wKjt4MBWRwHBL6MPwaxUQMGGIT3BDTT2wA=="},"y":{"dtype":"f4","bdata":"u0FxQAc59kAH3wdBITn4QCHgl0CgNolAu0FxQA=="},"type":"scatter"},{"hoverinfo":"skip","line":{"dash":"dash","width":1},"mode":"lines","name":"Reading Order","visible":"legendonly","x":{"dtype":"f4","bdata":"M4iVwFDc8UDbqB9A15spwFLu5kBp4kjAyaaJPq00A0EjR51A8+imwBNxMMFpfbPAWb0nQGKbakDtHtU\u002fEyS4PwzduMCqBIZAW6EqwGB5A0FIfxJAaVIiwd5qskCHhP9AMP6FQJs7jkBWOvLAcHG7QBqM6UD8\u002fh5B0NNLQJljdkC9gArAfRQEwYjnbz\u002ffWzI\u002fwQ4CwJ\u002f7EcF6Z7W\u002f5DKAwHYkn0Co7eDADTT2wOnPKcFg4fC\u002fl4jwvZmP58As7\u002f0\u002f2UaeQDwSbUBcFaNAAjX8P0h08UAjLAlBncNaQBh8LcB6np9A40IbQQHzzUBhMJxA92apQOGU4sCRMAhAU+J4wPzTDcCsVEDBhiE9wfk5P8ADfthAHi1EQPDQBD888ixBr7wDQXFBH8AjqhxB3LxnQJzxAUGj6T1AN66QvqY8QMECvijAYCwBwZAIOsFWRwHBL6MPwQ=="},"y":{"dtype":"f4","bdata":"7Bi7v\u002fZK20CTpoZA\u002fc\u002fiQHINRUDzSHI\u002fL4+nv9NfpUCyifZAk9X5vxUvrMDDkqA+\u002fuwsv5f208Dm9ZdAyrphwCvFl7\u002fZ6tVAJSg+wH4alkAASC3A6iBVv3iO9sAkGDvAOFohwQTZSsBeAsw\u002fWlywwPX31cDUUr7AMaArQVlFKcEKBU3AAd3gQMf85j+mE8c\u002ft10XQVJS1cB3LX1APsUnQUT23MAHOfZAu0FxQLWyHr7TngPB\u002f\u002fgWwcDx2L6AGTnB0qydvpi19EBwU47ADCXMvyNwiMB\u002ffwK\u002fm44cwEAo7MAITNA\u002fT2fPP0JoGj8QDDRA+DZGQEEK978AuLdACKlxv8DP5MAh4JdAoDaJQJrC0UD5vJ\u002fA+pD7P1XckkB0d3xAL0ftvvaJG0HtFnm\u002f69e1QBwnqz93zy3B+0q8wJVJpMDUyuQ\u002fANCoQAO1ksAH3wdBITn4QA=="},"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"\u003cb\u003eDim 1\u003c\u002fb\u003e"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"\u003cb\u003eDim 2\u003c\u002fb\u003e"}},"legend":{"title":{"text":"theme"},"tracegroupgap":0,"orientation":"h","yanchor":"bottom","y":-0.2,"xanchor":"center","x":0.5},"title":{"text":"\u003cb\u003et-SNE Dimension Reduction of Description Embeddings\u003c\u002fb\u003e","x":0.5},"margin":{"b":150},"annotations":[{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Control & Planning Models","x":4.199587345123291,"xanchor":"center","y":6.203699588775635,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Curricula & Scalable RL","x":5.401561260223389,"xanchor":"center","y":0.09145300090312958,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Open-Ended RL & Agents","x":3.13265061378479,"xanchor":"center","y":-7.417670249938965,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"PPO & RL Methods","x":-2.5970003604888916,"xanchor":"center","y":7.877668380737305,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Reasoning & LLM-RL","x":-6.789315223693848,"xanchor":"center","y":-1.7879666090011597,"yanchor":"middle"},{"font":{"color":"rgba(0,0,0,0.6)","family":"Arial","size":16},"opacity":0.8,"showarrow":false,"text":"Scaling & Architectures","x":-8.99372673034668,"xanchor":"center","y":6.1311774253845215,"yanchor":"middle"}]},                        {"responsive": true}                    )                };            </script>        </div>
</body>
</html>