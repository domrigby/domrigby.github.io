<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script>                <div id="f8b7c1d2-9c86-472a-80ce-4562ef8fff58" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("f8b7c1d2-9c86-472a-80ce-4562ef8fff58")) {                    Plotly.newPlot(                        "f8b7c1d2-9c86-472a-80ce-4562ef8fff58",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ecluster=2\u003cbr\u003et-SNE\u00a0dim\u00a01=%{x}\u003cbr\u003et-SNE\u00a0dim\u00a02=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["Absolute Zero Reasoner","Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment","Illusion of Thinking: Understanding Strengths and Limitations of Large Reasoning Models (LRMs)","Beyond the 80\u002f20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning","Writing-Zero: Bridge the Gap Between Non-Verifiable Tasks and Verifiable Rewards","Play to Generalize: Learning to Reason Through Game Play","Self-Adapting Language Models","Agents of Change: Self-Evolving LLM Agents for Strategic Planning","SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning","FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning"],"legendgroup":"2","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"2","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"k7CiQIwBh0CkapdAtEFMQIxMQkD1UY5AbpcnQG0bEEB8UYBARjuYQA=="},"xaxis":"x","y":{"dtype":"f4","bdata":"W3okwDmdJ7+mwck+8fHTPwMUhL8k+zfAp4WEvwvmhj5tdDrA\u002f\u002fyNPw=="},"yaxis":"y","type":"scatter"},{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ecluster=5\u003cbr\u003et-SNE\u00a0dim\u00a01=%{x}\u003cbr\u003et-SNE\u00a0dim\u00a02=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["Pi0.5","Ultimate Guide to Supervised Fine-Tuning","How Visual Representations Map to Language Feature Space in Multimodal LLMs"],"legendgroup":"5","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"5","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"gI87wCWFZ8AZZUrA"},"xaxis":"x","y":{"dtype":"f4","bdata":"F0OKwBUursDSlXfA"},"yaxis":"y","type":"scatter"},{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ecluster=1\u003cbr\u003et-SNE\u00a0dim\u00a01=%{x}\u003cbr\u003et-SNE\u00a0dim\u00a02=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["TD-MPC2","Synthetic Experience Replay","Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (Demo Augmented RL)","mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity","Prioritised Level Replay","KINETIX: INVESTIGATING THE TRAINING OF GENERAL AGENTS THROUGH OPEN-ENDED PHYSICS-BASED CONTROL TASKS","Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning","Self-Supervised Video Models Enable Understanding, Prediction and Planning (V-JEPA)"],"legendgroup":"1","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"1","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"6vSCPyBkqD9D6RO\u002fNIFaPu5NBEBXcI2+MYkHQC9f2L4="},"xaxis":"x","y":{"dtype":"f4","bdata":"XHPtwHRl2sAUu5\u002fAV2u0wLO+x8B0Mo7Azr6rwHLG88A="},"yaxis":"y","type":"scatter"},{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ecluster=4\u003cbr\u003et-SNE\u00a0dim\u00a01=%{x}\u003cbr\u003et-SNE\u00a0dim\u00a02=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["JointPPO","JaxMARL: Multi-Agent RL Environments and Algorithms in JAX","CHIRPs: Change-Induced Regret Proxy Metrics for Lifelong Reinforcement Learning","Training extremely large neural networks across thousands of GPUs by Jeremy Jordan","IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures","TorchBeast: A PyTorch Platform for Distributed RL","Distributed PPO Blog Post","Reinforcement Learning with Docker"],"legendgroup":"4","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"4","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"qMtHwKdg777mRRQ\u002f9qhLwDb14r9QTWi\u002fZ01LwI7uvj4="},"xaxis":"x","y":{"dtype":"f4","bdata":"gi3zPsBFqD+Ic08\u002fgo3av4KDHMAPOY6\u002fRB6yOzIcA0A="},"yaxis":"y","type":"scatter"},{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ecluster=0\u003cbr\u003et-SNE\u00a0dim\u00a01=%{x}\u003cbr\u003et-SNE\u00a0dim\u00a02=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["Spurious Rewards: Rethinking Training Signals in RLVR","ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models","Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions","Reinforcement Pre-Training","Reinforcement Learning Teachers of Test Time Scaling"],"legendgroup":"0","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"0","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"pI78P7z1pz\u002f4Vrs\u002fa7WHP2PqTz8="},"xaxis":"x","y":{"dtype":"f4","bdata":"KH3bv20DfL9eDmPASBIKwHH8BMA="},"yaxis":"y","type":"scatter"},{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ecluster=3\u003cbr\u003et-SNE\u00a0dim\u00a01=%{x}\u003cbr\u003et-SNE\u00a0dim\u00a02=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["Superintelligence From First Principles (blog post)","Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach","Automatic Curriculum Design for Zero-Shot Human AI Coordination","OMNI-EPIC: Open-Endedness Via Models of Human Notions of Interestingness With Environments Programmed In Code","Open-Endedness is Essential for Artificial Superhuman Intelligence"],"legendgroup":"3","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"3","orientation":"v","showlegend":true,"x":{"dtype":"f4","bdata":"2B+yQNKDUEArg3JAHgquQCCVqkA="},"xaxis":"x","y":{"dtype":"f4","bdata":"4+jLwK3HoMBqbMTA5iWpwEL20sA="},"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"t-SNE\u00a0dim\u00a01"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"t-SNE\u00a0dim\u00a02"}},"legend":{"title":{"text":"cluster"},"tracegroupgap":0},"title":{"text":"t-SNE of Paper Descriptions","x":0.5}},                        {"responsive": true}                    )                };            </script>        </div>
</body>
</html>